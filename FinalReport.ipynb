{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradelang <br>\n",
    "<br>\n",
    "Thomas Howard III <br>\n",
    "Mary Wishart <br>\n",
    "Brittany Lewis <br>\n",
    "<br>\n",
    "Introduction: <br>\n",
    "<br>\n",
    "Grade is an autograding framework for Python which provides more flexibility than traditional Python TestCases \n",
    "by allowing for arbitrarily complex testing conditions. Gradelang is a domain specific language for the Grade framework.\n",
    "It allows graders to create question structures to evaluate student executables. \n",
    "<br> <br>\n",
    "By creating Gradelang, we hope it will make it easier for teachers and graders to evaluate student code quickly, easily,\n",
    "and fairly. We also hope that the ease and flexibility of testing will create more space for creativity in assignment\n",
    "design. <br>\n",
    "<br>\n",
    "<br>\n",
    "Implementation: <br>\n",
    "<br>\n",
    "<br>\n",
    "Gradelang is broken up into block structures. There are four main types of blocks: <br>\n",
    "<br>\n",
    "- Setup: Statements in the setup block run before every question. Setup will most likely involve statements that create necessary test files or check requirements for questions. <br>\n",
    "- Teardown: Statements in the teardown block run after every question. This could include operations such as cleaning up created files by deleting them. <br>\n",
    "- Question: Each question block runs independently from each other question and allows the grader to specify tests and award values associated with those questions. <br>\n",
    "- Output: This block determines how the results of our program are outputted. <br>\n",
    "<br>\n",
    "Within those structures we support common arithmetic expressions as well as statements for file creation, \n",
    "variable assignment, arbitrary string, float, and int creation through the hypothesis framework. Most importantly\n",
    "tests and be run and tested through the run and assert commands which are supported through the Grade pipeline <br>\n",
    "<br>\n",
    "<br>\n",
    "Challenges: <br>\n",
    "<br>\n",
    "One challenge we faced was making each question truly independent. In order to ensure that one question test failure\n",
    "would not affect any future tests (an important requirement for autograding) we run each question in a separate subprocess. <br>\n",
    "<br>\n",
    "This can cause issues for python 3.7.3 and python version 3.7.4 both of which have a bug within their subprocess handling. <br>\n",
    "<br>\n",
    "If you have issues with your subprocess run, please try updating your python installation to python 3.8. Further\n",
    "instructions and a docker image are also included. Please contact the project team with any problems you encounter. <br>\n",
    "<br>\n",
    "In addition to these challenges, we also had some challenges with getting hypothesis to work with the structure we wanted\n",
    "for our language. <br>\n",
    "\n",
    "\n",
    "Examples: <br>\n",
    "<br>\n",
    "In order to run these examples you will need to have: <br>\n",
    "<br>\n",
    "- Python version 3.8\n",
    "- ply\n",
    "    - pip install ply\n",
    "- Grade \n",
    "    - pip install grade\n",
    "    - https://github.com/thoward27/grade\n",
    "- Hypothesis\n",
    "    - pip install Hypothesis\n",
    "    - https://hypothesis.readthedocs.io/en/latest/\n",
    "    \n",
    "In addition to the below examples we have a full suite of python unittests in the test folder of our project. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradelang.interpreter import interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Results\n",
      "Grade Results\n",
      "Grade Results\n"
     ]
    }
   ],
   "source": [
    "# Basic setup tests\n",
    "empty = \"setup {}\"\n",
    "interpret(empty)\n",
    "\n",
    "trivial_passing = \"setup { assert 1 == 1; }\"\n",
    "interpret(trivial_passing)\n",
    "\n",
    "trivial_failing = \"setup { assert 1 == 0; }\"\n",
    "interpret(trivial_failing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Results\n",
      "Question 0: 0/0.\n",
      "Grade Results\n",
      "Question 0: 0/0.\n",
      "Grade Results\n",
      "Question 0: 0/0.\n",
      "Exception thrown: AssertionError(\"False was not true! ('==', ('integer', 0), ('integer', 1))\",)\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/interpreter.py\", line 63, in worker\n",
      "    walk(question.body)\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/walk.py\", line 151, in walk\n",
      "    return dispatch[action](ast)\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/walk.py\", line 133, in <lambda>\n",
      "    'seq': lambda ast: (walk(ast[1]), walk(ast[2])),\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/walk.py\", line 151, in walk\n",
      "    return dispatch[action](ast)\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/walk.py\", line 60, in <lambda>\n",
      "    'assert': lambda ast: __assert(walk(ast[1]), ast[1]),\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/walk.py\", line 50, in __assert\n",
      "    raise AssertionError(f'{cond} was not true! {message}')\n",
      "AssertionError: False was not true! ('==', ('integer', 0), ('integer', 1))\n",
      "\n",
      "Grade Results\n",
      "Question 0: 0/0.\n",
      "Grade Results\n",
      "Question 0: 0/0.\n",
      "Grade Results\n",
      "Question string: 10/10.\n",
      "Grade Results\n",
      "Question 0: 0/0.\n",
      "Grade Results\n",
      "Question named: 0/0.\n",
      "Grade Results\n",
      "Question 1: 0/0.\n"
     ]
    }
   ],
   "source": [
    "#Question tests\n",
    "\n",
    "empty = \"question {}\"\n",
    "interpret(empty)\n",
    "\n",
    "trivial_passing = \"question { assert 1 == 1; }\"\n",
    "interpret(trivial_passing)\n",
    "\n",
    "trivial_failing = \"question { assert 0 == 1; }\"\n",
    "interpret(trivial_failing)\n",
    "\n",
    "testing_output = \"\"\"\n",
    "question {\n",
    "    run \"echo hello world\";\n",
    "    assert \"hello world\" in stdout;\n",
    "}\n",
    "\"\"\"\n",
    "interpret(testing_output)\n",
    "\n",
    "testing_exit_success = \"\"\"\n",
    "question {\n",
    "    run \"echo hello world\";\n",
    "    assert exit successful;\n",
    "}\n",
    "\"\"\"\n",
    "interpret(testing_exit_success)\n",
    "\n",
    "testing_create_string=\"\"\"\n",
    "    question \\\"string\\\"{\n",
    "        let x be String();\n",
    "        run \"echo\", x;\n",
    "        assert x in stdout;\n",
    "        award 10;\n",
    "    }\n",
    "\"\"\"\n",
    "interpret(testing_create_string)\n",
    "\n",
    "interpret(testing_exit_success)\n",
    "\n",
    "name_string = 'question \"named\" {}'\n",
    "interpret(name_string)\n",
    "\n",
    "name_int = 'question 1 {}'\n",
    "interpret(name_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Results\n",
      "Grade Results\n",
      "Grade Results\n"
     ]
    }
   ],
   "source": [
    "#Teardown tests\n",
    "empty = \"teardown {}\"\n",
    "interpret(empty)\n",
    "\n",
    "trivial_passing = \"teardown { assert 1 == 1; }\"\n",
    "interpret(trivial_passing)\n",
    "\n",
    "trivial_failing = \"teardown { assert 0 == 1; }\"\n",
    "interpret(trivial_failing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Results\n",
      "{\n",
      "    \"tests\": []\n",
      "}\n",
      "# Results\n",
      "## Score: 0/0\n"
     ]
    }
   ],
   "source": [
    "#Output tests\n",
    "empty = \"output {}\"\n",
    "interpret(empty)\n",
    "\n",
    "json = 'output { json; }'\n",
    "interpret(json)\n",
    "\n",
    "markdown = 'output { markdown; }'\n",
    "interpret(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Results\n",
      "Question 0: 0/0.\n",
      "Grade Results\n",
      "Question 0: 0/0.\n",
      "Exception thrown: AssertionError(\"False was not true! ('==', ('integer', 1), ('integer', 0))\",)\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/interpreter.py\", line 62, in worker\n",
      "    walk(setup)\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/walk.py\", line 151, in walk\n",
      "    return dispatch[action](ast)\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/walk.py\", line 133, in <lambda>\n",
      "    'seq': lambda ast: (walk(ast[1]), walk(ast[2])),\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/walk.py\", line 151, in walk\n",
      "    return dispatch[action](ast)\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/walk.py\", line 60, in <lambda>\n",
      "    'assert': lambda ast: __assert(walk(ast[1]), ast[1]),\n",
      "  File \"/mnt/c/users/marye/Documents/GitHub/gradelang/gradelang/walk.py\", line 50, in __assert\n",
      "    raise AssertionError(f'{cond} was not true! {message}')\n",
      "AssertionError: False was not true! ('==', ('integer', 1), ('integer', 0))\n",
      "\n",
      "Grade Results\n",
      "Question 1: 10/10.\n",
      "Question 2: 20/20.\n",
      "Question 3: 50/50.\n"
     ]
    }
   ],
   "source": [
    "#Testing programs\n",
    "empty =\"\"\"\n",
    "setup {}\n",
    "question {}\n",
    "teardown {}\n",
    "output {}\n",
    "\"\"\"\n",
    "interpret(empty)\n",
    "\n",
    "setup_failure = \"\"\"\n",
    "setup { assert 1 == 0; }\n",
    "question { assert 1 == 1; }\n",
    "teardown {}\n",
    "output {}\n",
    "\"\"\"\n",
    "interpret(setup_failure)\n",
    "\n",
    "proposal = \"\"\"\n",
    "        setup {\n",
    "            touch \"@/temp.txt\";\n",
    "            run \"echo\";\n",
    "            assert exit successful;\n",
    "        }\n",
    "\n",
    "        teardown {\n",
    "            remove \"@/temp.txt\";\n",
    "        }\n",
    "\n",
    "        output {\n",
    "            json;\n",
    "        }\n",
    "\n",
    "        output {\n",
    "        }\n",
    "            question 1 {\n",
    "            # Run the program, saving output.\n",
    "            run \"echo\", \"hello world\";\n",
    "\n",
    "            # Now let's run some checks.\n",
    "            assert exit successful;\n",
    "\n",
    "            # This checks both stdout and stderr\n",
    "            assert \"hello\" in stdout;\n",
    "\n",
    "            award 10;\n",
    "        }\n",
    "\n",
    "        question 2  {\n",
    "            run \"echo\", \"hello world\";\n",
    "            assert \"goodbye\" not in stdout;\n",
    "            award 10;\n",
    "            assert \"hello\" in stdout;\n",
    "            assert \"hello\" not in stderr;\n",
    "            award 10;\n",
    "        }\n",
    "\n",
    "        question 3 {\n",
    "            let x be Float(minvalue=1);\n",
    "            run \"echo\", x;\n",
    "\n",
    "            # If we want to just look at stdout.\n",
    "            assert x in stdout;\n",
    "            \n",
    "            String y = \"fish\";\n",
    "            run \"echo\", y;\n",
    "            assert \"fish\" in stdout;\n",
    "            \n",
    "            let z be String();\n",
    "            run \"echo\", z;\n",
    "            assert z in stdout;\n",
    "            \n",
    "            let camel be Int(min_value=6);\n",
    "            run \"echo\", camel;\n",
    "            assert camel in stdout;\n",
    "            \n",
    "            award 50;\n",
    "        }\n",
    "        \"\"\"\n",
    "interpret(proposal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions: <br>\n",
    "<br>\n",
    "We successfully created a simple, easy to use language to work with the Grade autograding framework. We believe that this\n",
    "will make it easier to efficiently grade assignments. We also think our language will be natural and easy to learn\n",
    "for anyone who has used an autograder in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Results\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
